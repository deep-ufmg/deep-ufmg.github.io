{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5frFFEPqDCpE"
   },
   "source": [
    "---\n",
    "layout: page\n",
    "title: Regressão Logística e SVM\n",
    "nav_order: 3\n",
    "---\n",
    "\n",
    "# Regressão Logística e SVM\n",
    "\n",
    "> - Regressão Logística\n",
    "> - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joe0cmXnED64"
   },
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3V3xThND-vT"
   },
   "source": [
    "### Exercício 1\n",
    "\n",
    "No modelo de regressão logística com um regressor apenas, a probabilidade é definida como:\n",
    "\n",
    "$$ P(Y_i = 1) = p(x_i) = \\frac{1}{1 + \\exp(-b - w_1 x_i)}. $$\n",
    "\n",
    "Deduza que a log-verossimilhança de $ \\theta = (b, w_1) $ no caso do modelo logístico com um único regressor é dada por:\n",
    "\n",
    "$$\n",
    "l(\\theta) = b \\sum_{i=1}^n y_i + w_1 \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n \\log(1 + e^{b + w_1 x_i}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z2AU_PrD_Wf"
   },
   "source": [
    "**Resposta:**\n",
    "\n",
    "Para deduzir a expressão da log-verossimilhança $ l(\\theta) $ no modelo de regressão logística com um regressor, começamos pela definição da função de verossimilhança para variáveis aleatórias Bernoulli. Cada observação $ Y_i $ pode assumir os valores 0 ou 1, com probabilidades $ 1 - p_i $ e $ p_i $, respectivamente, onde:\n",
    "\n",
    "$$\n",
    "p_i = P(Y_i = 1) = \\frac{1}{1 + \\exp(-b - w_1 x_i)}.\n",
    "$$\n",
    "\n",
    "A função de verossimilhança é dada por:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i}.\n",
    "$$\n",
    "\n",
    "Tomando o log da verossimilhança, obtemos a log-verossimilhança:\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^n  y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) .\n",
    "$$\n",
    "\n",
    "Agora, expressamos $ \\log(p_i) $ e $ \\log(1 - p_i) $ em termos do bias $ b $ e do peso $ w_1 $:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_i &= \\frac{e^{b + w_1 x_i}}{1 + e^{b + w_1 x_i}} \\\\\n",
    "\\log(p_i) &= (b + w_1 x_i) - \\log(1 + e^{b + w_1 x_i}) \\\\\n",
    "\\log(1 - p_i) &= -\\log(1 + e^{b + w_1 x_i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Repare que $log(e^x) = x$\n",
    "\n",
    "Substituindo essas expressões na log-verossimilhança:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "l(\\theta) &= \\sum_{i=1}^n  y_i ((b + w_1 x_i) - \\log(1 + e^{b + w_1 x_i})) + (1 - y_i)( -\\log(1 + e^{b + w_1 x_i}))  \\\\\n",
    "&= \\sum_{i=1}^n  y_i (b + w_1 x_i) - \\log(1 + e^{b + w_1 x_i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Separando os somatórios, temos:\n",
    "\n",
    "$$\n",
    "l(\\theta) = b \\sum_{i=1}^n y_i + w_1 \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n \\log(1 + e^{b + w_1 x_i}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS0Zr9WbDOo9"
   },
   "source": [
    "\n",
    "### Exercício 2\n",
    "\n",
    "No modelo logístico com um regressor apenas, mostre que o vetor gradiente da log-verossimilhança $ l(\\theta) $ é dado por:\n",
    "\n",
    "$$\n",
    "\\nabla l(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial b} \\\\\n",
    "\\frac{\\partial l}{\\partial w_1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n y_i - p_i \\\\\n",
    "\\sum_{i=1}^n x_i y_i - p_i x_i\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "onde $ p_i = p(x_i) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObR4njKFEGLQ"
   },
   "source": [
    "**Resposta**\n",
    "\n",
    "\n",
    "Para encontrar o vetor gradiente da log-verossimilhança $ l(\\theta) $, calculamos as derivadas parciais em relação a $ b $ e $ w_1 $:\n",
    "\n",
    "**Derivada parcial em relação a $ b $:**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial b} &= \\sum_{i=1}^n \\left[ y_i - \\frac{e^{b + w_1 x_i}}{1 + e^{b + w_1 x_i}} \\right] * \\\\\n",
    "&= \\sum_{i=1}^n \\left( y_i - p_i \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "> *Olhe a questão acima para ver essa derivação de $p_i$\n",
    "\n",
    "**Derivada parcial em relação a $w_1$:**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial w_1} &= \\sum_{i=1}^n \\left[ y_i x_i - \\frac{e^{b + w_1 x_i}}{1 + e^{b + w_1 x_i}} x_i \\right] \\\\\n",
    "&= \\sum_{i=1}^n \\left( y_i x_i - p_i x_i \\right) \\\\\n",
    "&= \\sum_{i=1}^n x_i \\left( y_i - p_i \\right).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Portanto, o vetor gradiente é:\n",
    "\n",
    "$$\n",
    "\\nabla l(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n \\left( y_i - p_i \\right) \\\\\n",
    "\\sum_{i=1}^n x_i \\left( y_i - p_i \\right)\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fWX3Q1gEErB"
   },
   "source": [
    "\n",
    "### Exercício 3\n",
    "\n",
    "Ainda no modelo logístico com um regressor apenas, mostre que a matriz Hessiana da log-verossimilhança $l(\\theta) $ é dada por:\n",
    "\n",
    "$$\n",
    "H(l(\\theta)) =\n",
    "- \\begin{bmatrix}\n",
    "\\frac{\\partial^2 l}{\\partial b^2} & \\frac{\\partial^2 l}{\\partial b \\partial w_1} \\\\\n",
    "\\frac{\\partial^2 l}{\\partial b \\partial w_1} & \\frac{\\partial^2 l}{\\partial w_1^2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "- \\begin{bmatrix}\n",
    "\\sum_{i=1}^n p_i (1 - p_i) & \\sum_{i=1}^n p_i (1 - p_i) x_i \\\\\n",
    "\\sum_{i=1}^n p_i (1 - p_i) x_i & \\sum_{i=1}^n p_i (1 - p_i) x_i^2\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jay9wv8KEIAh"
   },
   "source": [
    "**Resposta**\n",
    "\n",
    "Para determinar a matriz Hessiana da log-verossimilhança $l(\\theta)$, calculamos as segundas derivadas parciais:\n",
    "\n",
    "**Derivada segunda em relação a $b$:**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 l}{\\partial b^2} &= -\\sum_{i=1}^n \\frac{e^{b + w_1 x_i}}{(1 + e^{b + w_1 x_i})^2} \\\\\n",
    "&= -\\sum_{i=1}^n p_i (1 - p_i).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Derivada mista em relação a $b $ e $w_1$:**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 l}{\\partial b \\partial w_1} &= -\\sum_{i=1}^n \\frac{e^{b + w_1 x_i}}{(1 + e^{b + w_1 x_i})^2} x_i \\\\\n",
    "&= -\\sum_{i=1}^n p_i (1 - p_i) x_i.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Derivada segunda em relação a $w_1$:**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 l}{\\partial w_1^2} &= -\\sum_{i=1}^n \\frac{e^{b + w_1 x_i}}{(1 + e^{b + w_1 x_i})^2} x_i^2 \\\\\n",
    "&= -\\sum_{i=1}^n p_i (1 - p_i) x_i^2.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Assim, a matriz Hessiana é:\n",
    "\n",
    "$$\n",
    "H(l(\\theta)) =\n",
    "- \\begin{bmatrix}\n",
    "\\sum_{i=1}^n p_i (1 - p_i) & \\sum_{i=1}^n p_i (1 - p_i) x_i \\\\\n",
    "\\sum_{i=1}^n p_i (1 - p_i) x_i & \\sum_{i=1}^n p_i (1 - p_i) x_i^2\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCtt0hDzHeZu"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZWEq2dzHrXS"
   },
   "source": [
    "### Exercício 4\n",
    "\n",
    "Quais as principais diferenças entre o SVM e uma Regressão Linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcWD4mkbH1Fg"
   },
   "source": [
    "**Resposta**\n",
    "\n",
    "Existem duas principais diferenças entre o SVM e a Regressão Linear.\n",
    "\n",
    "1. Enquanto a regressão linear encontra uma fronteira de decisão apenas minimizando o erro quadrático, o SVM tenta encontrar o melhor separador, para isso maximizando a margem entre os vetores de suporte.\n",
    "\n",
    "2. O Truque do Kernel possibilita que o SVM encontre soluções de problemas não lineares. Embora possamos usar features polinomiais na regressão linear, o truque do kernel nos permite atingir o mesmo resultado sem calcular diretamente as features no espaço com mais dimensões, de forma que os cálculos são mais rápidos e eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGhdSRIkH4YY"
   },
   "source": [
    "### Exercício 5\n",
    "\n",
    "No SVM, quais são os tipos de kernels mais comuns? Como eles são definidos? Em quais situações usamos cada um deles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPs8t9oqIFYc"
   },
   "source": [
    "**Resposta**\n",
    "\n",
    "1. **Kernel Linear**:\n",
    "  - Definição:\n",
    "  $$ K(x, x') = x \\cdot x'$$\n",
    "  - Usado para problemas linearmente separáveis.\n",
    "2. **Kernel Polinomial**:\n",
    "  - Definição:\n",
    "  $$ K(x, x') = (x \\cdot x' + c)^d$$\n",
    "  - Bom para dados com características polinomiais. É muito flexível, uma vez que podemos alterar o parâmetro $d$, porém um $d$ grande pode ser bastante custoso.\n",
    "\n",
    "3. **Radial Basis Function (RBF)**\n",
    "\n",
    "  - Definição:\n",
    "  $$K(x, x') =  \\exp(- \\frac {||x - x'||^2}{2σ^2})$$\n",
    "  - Muito usado na prática. Mapeia os dados para um espço complexo com dimensões infinitas, possibilitando que ele modelo dados muito difíceis.\n",
    "\n",
    "4. **Kernel Sigmoid**\n",
    "  - Definição:\n",
    "  $$K(x, x') = tanh(αx ⋅ x' + c)$$\n",
    "  - Menos comum, útil em algumas poucas situações. Pode apresentar problemas de convergência, a depender dos valores de $\\alpha$ e $c$.\n",
    "\n",
    "\n",
    "De forma geral, o kernel a ser usado depende da natureza dos dados a serem processados. Escolher o melhor deles para o seu problema cegamente é difícil, e, na prática, é melhor testar várias opções e ver qual se encaixou melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqOaEsJBIG85"
   },
   "source": [
    "### Exercício 6\n",
    "\n",
    "O que é uma *Soft Margin* em um SVM? Por que ela é usada nos SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWsQOd-7IXgH"
   },
   "source": [
    "A Soft Margin no SVM possibilita que o modelo encontre uma margem não estrita, isso é, com alguns pontos dentro dela, ou até classificados erroneamente. Ela é controlada pelo parâmetro c, quanto mais baixo, mais relaxada a margem será. O ajuste da soft margin é essencial no uso do SVM, já que, na prática, os problemas com os quais lidamos não são perfeitamente separáveis. Assim, para garantir que o modelo encontre uma fronteire de decisão boa, que seja capaz de generalizar para novos dados, precisamos permitir que ele 'abandone' alguns pontos ao definir a margem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY2BMdTkDON1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
