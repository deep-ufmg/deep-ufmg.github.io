{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL024U_Co35T"
   },
   "source": [
    "# Lista Teórica 03 - Modelos de Linguagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_0rf1URpiqZ"
   },
   "source": [
    "### Exercício 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TdO0j3HpQBN"
   },
   "source": [
    "Considere o seguinte exemplo:\n",
    "\n",
    "Dados de treino:\n",
    "\n",
    "```\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> Sam I like </s>\n",
    "<s> Sam I do like </s>\n",
    "<s> do I like Sam </s>\n",
    "```\n",
    "\n",
    "Imagine que estamos treinando um modelo de linguagem bi-grama com os\n",
    "dados de treino mostrados acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EarFgyhjzr4n"
   },
   "source": [
    "Probabilidade de cada bi-grama:\n",
    "\n",
    "$P(Sam| < s >) = \\frac{3}{5}$\n",
    "\n",
    "$P(I| < s >) = \\frac{1}{5}$\n",
    "\n",
    "$P(I|Sam) = \\frac{3}{5}$\n",
    "\n",
    "$P(< /s > |Sam) = \\frac{2}{5}$\n",
    "\n",
    "$P(Sam|am) = \\frac{1}{2}$\n",
    "\n",
    "$P(< /s > |am) = \\frac{1}{2}$\n",
    "\n",
    "$P(am|I) = \\frac{2}{5}$\n",
    "\n",
    "$P(like|I) = \\frac{2}{5}$\n",
    "\n",
    "$P(do|I) = \\frac{1}{5}$\n",
    "\n",
    "$P(Sam|like) = \\frac{1}{3}$\n",
    "\n",
    "$P(< /s > |like) = \\frac{2}{3}$\n",
    "\n",
    "$P(like|do) = \\frac{1}{2}$\n",
    "\n",
    "$P(I|do) = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUmvloxKqz84"
   },
   "source": [
    "Qual seria a próxima palavra mais provável de ser predita pelo modelo\n",
    "para cada uma das sequências de palavras abaixo?\n",
    "\n",
    "a)   `<s> Sam ...`\n",
    "\n",
    "b)   `<s> Sam I do ...`\n",
    "\n",
    "c)   `<s> Sam I am Sam ...`\n",
    "\n",
    "d)   `<s> do I like ...`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOEtw2G_2qPF"
   },
   "source": [
    "**Resposta:**\n",
    "\n",
    "a) “I”.\n",
    "\n",
    "b) “I” e “like” são igualmente prováveis.\n",
    "\n",
    "c) “I”.\n",
    "\n",
    "d) $</s>$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWHCtMIkrb9V"
   },
   "source": [
    "Observe essas três sentenças:\n",
    "\n",
    "a) `<s> Sam I do I like </s>`\n",
    "\n",
    "b) `<s> Sam I am </s>`\n",
    "\n",
    "c) `<s> I do like Sam I am </s>.`\n",
    "\n",
    "Qual delas é mais provável de acordo com esse modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adj7PHEt1iXH"
   },
   "source": [
    "**Resposta:**\n",
    "\n",
    "a: 3/5 × 3/5 × 1/5 × 1/2 × 2/5 × 2/3\n",
    "\n",
    "b: 3/5 × 3/5 × 2/5 × 1/2\n",
    "\n",
    "c: 1/5 × 1/5 × 1/2 × 1/3 × 3/5 × 2/5 × 1/2\n",
    "\n",
    "(b) é a sentença mais provável de acordo com o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBkTZA7Brr07"
   },
   "source": [
    "### Exercício 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW60qYKOruuG"
   },
   "source": [
    "Cite as principais desvantagens dos modelos n-grama e possíveis soluções\n",
    "para esses problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC5AGXsf3gXI"
   },
   "source": [
    "**Resposta:**\n",
    "\n",
    "Problemas com modelos n-gramas: À medida que n aumenta, é necessário mais memória para armazenar os n-gramas e, além disso, as probabilidades tendem a ser cada vez menores dado o contexto cada vez mais específico. Esparsidade dos dados: a maioria dos n-gramas nunca aparece no corpus, mesmo quando são possíveis (e.g.: e se “estudantes abriram as mochilas” nunca apareceu nos dados?) dessa forma, a probabilidade seria zero.\n",
    "\n",
    "Algumas maneiras de lidar com esparsidade dos dados são: usar contexto menor (trade-off: modelo menos poderoso); suavizar probabilidades (e.g., adicionando ocorrências imaginárias); prevendo com um ensemble de modelos n-grama com n diferentes; ou utilizar modelos distribuídos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1HsG0Garywc"
   },
   "source": [
    "### Exercício 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIkqJN9Jr1bz"
   },
   "source": [
    "Descreve brevemente como funcionam os modelos neurais de linguagem e\n",
    "o que são representações *one-hot* e os *embeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m9bDoVR3x8o"
   },
   "source": [
    "**Resposta:**\n",
    "\n",
    "Prever a distribuição da próxima palavra dadas as K anteriores é apenas um problema de classificação (multi-classe). Nesse caso, usamos um 1-of-K (one-hot) encoding para palavras, a primeira camada pode ser vista como uma camada com pesos amarrados. *One-hot encoding* são representações esparsas (dimensão: $n_v × 1$, onde $n_v$ é igual ao tamanho do vocabulário) usadas para representar cada palavra do vocabulário. A matriz de pesos age como uma lookup table (seleção de coluna) onde cada coluna é a representação de uma palavra, aka embedding, feature vector ou encoding. *Embeddings* são representações densas (dimensão: $n_e × 1$, onde $n_e$ é igual ao tamanho do embedding escolhido) que enfatizam a localização de uma palavra em um espaço de alta dimensão onde palavras próximas são mais similares semanticamente."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+B7VXL6nDgeJ5qb4+e0lE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
